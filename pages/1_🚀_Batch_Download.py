import streamlit as st
import pandas as pd
import time
import plotly.express as px
from datetime import datetime, timedelta
import config
import utils # å¼•ç”¨å…¬å…±åº“

st.set_page_config(page_title="Data Download / æ‰¹é‡ä¸‹è½½", page_icon="ğŸš€", layout="wide")

st.title("ğŸš€ Batch Download Center (æ‰¹é‡ä¸‹è½½ä¸­å¿ƒ)")

# åˆå§‹åŒ–çŠ¶æ€
if 'show_heatmap' not in st.session_state:
    st.session_state['show_heatmap'] = False

# --- ä¾§è¾¹æ  (ç‹¬ç«‹äºä¸»é¡µ) ---
with st.sidebar:
    st.header("âš™ï¸ Settings (å‚æ•°è®¾ç½®)")
    # è‡ªåŠ¨è·å– Token
    token = utils.get_auto_token()
    if token:
        remaining = int((st.session_state['token_expiry'] - time.time()) / 60)
        st.success(f"âœ… API Connected (å‰©ä½™ {remaining} min)")
    else:
        st.error("âŒ Connection Failed (è¿æ¥å¤±è´¥)")
        
    st.divider()
    selected_category = st.selectbox("Product Group (äº§å“åˆ†ç±»)", list(config.HS_CODES_MAP.keys()))
    target_hs_codes = config.HS_CODES_MAP[selected_category]

# --- ç•Œé¢ ---
st.markdown("### ğŸ› ï¸ Task Configuration (ä»»åŠ¡é…ç½®)")

c_dl1, c_dl2 = st.columns(2)
with c_dl1: 
    st.caption("Quick Select - Origin (å¿«æ·é€‰æ‹©-å‡ºå£å›½):")
    utils.render_region_buttons("dl_o", c_dl1)
    dl_origins = st.multiselect("Exporting Countries (å‡ºå£å›½)", utils.get_all_country_codes(), format_func=utils.country_format_func, key="dl_o")
with c_dl2: 
    st.caption("Quick Select - Dest (å¿«æ·é€‰æ‹©-è¿›å£å›½):")
    utils.render_region_buttons("dl_d", c_dl2)
    dl_dests = st.multiselect("Importing Countries (è¿›å£å›½)", utils.get_all_country_codes(), format_func=utils.country_format_func, key="dl_d")

c_api1, c_api2, c_api3 = st.columns(3)
with c_api1: selected_hs = st.multiselect("HS Codes (æµ·å…³ç¼–ç )", target_hs_codes, key="dl_h")
with c_api2: 
    species_options = list(config.SPECIES_KEYWORDS.keys()) + ["Other", "Unknown"]
    dl_species = st.multiselect("Species Filter (æ ‘ç§ - APIç­›é€‰)", species_options, key="dl_sp", help="é€‰ä¸­åï¼ŒAPIè¯·æ±‚å°†åªè¿”å›åŒ…å«è¿™äº›æ ‘ç§å…³é”®è¯çš„æ•°æ®ã€‚")
with c_api3: selected_dirs = st.multiselect("Trade Flow (è´¸æ˜“æ–¹å‘)", ["imports", "exports"], key="dl_dr")

final_hs = selected_hs if selected_hs else target_hs_codes
final_dirs = selected_dirs if selected_dirs else ["imports", "exports"]

# --- å…³é”®è¯ç”Ÿæˆé€»è¾‘ ---
api_keyword_str = None
if dl_species:
    # æå–é€‰ä¸­æ ‘ç§çš„ç¬¬ä¸€ä¸ªå…³é”®è¯ï¼Œä½œä¸º API æœç´¢è¯
    kws = []
    for s in dl_species:
        if s in config.SPECIES_KEYWORDS:
            # å–è¯¥æ ‘ç§é…ç½®åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªè¯ (ä¾‹å¦‚ Radiata -> RADIATA)
            kws.append(config.SPECIES_KEYWORDS[s][0])
    
    if len(kws) > 1:
        # å¦‚æœé€‰äº†å¤šä¸ªï¼Œç”¨ç©ºæ ¼è¿æ¥ã€‚è­¦å‘Šç”¨æˆ· API å¯èƒ½å°†å…¶è§†ä¸º AND å…³ç³»
        api_keyword_str = " ".join(kws)
        st.warning(f"âš ï¸ Multi-Species Filter: Searching for '{api_keyword_str}'. (API likely treats this as 'AND' logic. For volume check, recommend selecting ONE species at a time.)")
    elif kws:
        # å•ä¸ªé€‰æ‹©ï¼Œæ­£å¸¸æœç´¢
        api_keyword_str = kws[0]
        st.success(f"ğŸ§¬ Species Filter Active: '{api_keyword_str}' (Will be applied to API requests)")

st.divider()

# --- 1. æœ¬åœ°åº“å­˜æ£€æŸ¥ ---
st.markdown("#### 1ï¸âƒ£ Local Stock Check (æœ¬åœ°åº“å­˜æ£€æŸ¥)")

c_inv_yr, c_inv_btn = st.columns([1, 2])
with c_inv_yr:
    check_year = st.selectbox("Select Year (é€‰æ‹©å¹´ä»½)", [2024, 2025, 2026], index=2, key="check_year_box")

with c_inv_btn:
    st.write("") 
    st.write("") 
    if st.button("ğŸ“Š Show Heatmap (æ˜¾ç¤ºåº“å­˜çƒ­åŠ›å›¾)"):
        st.session_state['show_heatmap'] = True
        st.rerun()

if st.session_state.get('show_heatmap', False):
    st.divider()
    check_start = f"{check_year}-01-01"
    check_end = f"{check_year}-12-31"
    sp_msg = f"Species: {dl_species}" if dl_species else "Species: All"
    
    with st.spinner(f"Scanning Database for {check_year}... (æ­£åœ¨æ‰«ææ•°æ®åº“)"):
        # æœ¬åœ°æ£€æŸ¥ä¾ç„¶ä½¿ç”¨ç²¾ç¡®çš„ Python é€»è¾‘
        coverage_df = utils.check_data_coverage(final_hs, check_start, check_end, origin_codes=dl_origins, dest_codes=dl_dests, target_species_list=dl_species)
        
        if not coverage_df.empty:
            full_range = pd.date_range(start=check_start, end=check_end)
            full_df = pd.DataFrame({'date': full_range}).merge(coverage_df, on='date', how='left').fillna(0)
            
            fig = px.scatter(
                full_df, x="date", y=[1]*len(full_df), 
                size="count", color="count", 
                color_continuous_scale=["#e0e0e0", "green"], 
                title=f"Stock Distribution {check_year} (åº“å­˜åˆ†å¸ƒ) | Total: {coverage_df['count'].sum()} records", 
                height=250
            )
            fig.update_yaxes(visible=False, showticklabels=False)
            fig.update_layout(plot_bgcolor='white', xaxis=dict(showgrid=False))
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning(f"âš ï¸ No Data found for {check_year} (è¯¥å¹´ä»½æ— æ•°æ®)")
    
    if st.button("âŒ Close Chart (å…³é—­å›¾è¡¨)"):
        st.session_state['show_heatmap'] = False
        st.rerun()

st.divider()

# --- 2. API é¢„æ£€ ---
st.markdown("#### 2ï¸âƒ£ API Volume Check (API é¢„æ£€)")

dl_date_range = st.date_input("Date Range (ä¸‹è½½æ—¥æœŸèŒƒå›´)", value=(datetime.today() - timedelta(days=7), datetime.today()), key="dl_date_key")

if st.button("ğŸ” Check Volume (æŸ¥è¯¢æ•°æ®é‡)"):
    with st.status(f"Querying Tendata API... (Keyword: {api_keyword_str if api_keyword_str else 'None'})", expanded=True) as status:
        if not token: status.update(label="Auth Failed (è®¤è¯å¤±è´¥)", state="error"); st.stop()
        results = []
        total_count = 0
        for hs in final_hs:
            for d in final_dirs:
                # è°ƒç”¨ utils, ä¼ å…¥ keyword
                res = utils.fetch_tendata_api(hs, dl_date_range[0], dl_date_range[1], token, d, dl_origins, dl_dests, just_checking=True, keyword=api_keyword_str)
                if res and str(res.get('code')) == '200':
                    data_node = res.get('data', {})
                    count = data_node.get('total', 0)
                    if count == 0: count = data_node.get('totalElements', 0)
                    results.append({"HS Code": hs, "Flow": d, "API Count": count})
                    total_count += count
                else:
                    # [ä¿®å¤] æ˜¾ç¤ºå…·ä½“çš„é”™è¯¯ä¿¡æ¯ï¼Œæ–¹ä¾¿äº‘ç«¯æ’æŸ¥
                    error_msg = res.get('msg', 'Unknown Error') if res else 'No Response'
                    error_code = res.get('code', 'N/A') if res else 'N/A'
                    results.append({"HS Code": hs, "Flow": d, "API Count": f"Err {error_code}: {error_msg}"})
                    
        status.update(label="Complete (å®Œæˆ)", state="complete")
        if results:
            st.table(pd.DataFrame(results))
            if total_count > 0: st.success(f"âœ… Total found on API: {total_count} records.")

# --- 3. æ‰§è¡Œä¸‹è½½ (åŒ…å«æ–­ç‚¹ç»­ä¼ é€»è¾‘) ---
st.markdown("#### 3ï¸âƒ£ Execute Download (æ‰§è¡Œä¸‹è½½)")

# [NEW] å¸ƒå±€è°ƒæ•´ï¼šå¢åŠ èµ·å§‹é¡µè¾“å…¥æ¡†
c_exec1, c_exec2 = st.columns([1, 4])
with c_exec1:
    start_page_val = st.number_input("Start Page (èµ·å§‹é¡µç )", min_value=1, value=1, help="ç”¨äºæ–­ç‚¹ç»­ä¼ ã€‚æ³¨æ„ï¼šæ­¤é¡µç å°†åº”ç”¨äºæ‰€æœ‰é€‰ä¸­çš„ HS Codeï¼Œå»ºè®®ç»­ä¼ æ—¶åªå‹¾é€‰å•ä¸ªä»»åŠ¡ã€‚")
with c_exec2:
    st.write("") # Spacer
    st.write("") # Spacer
    start_btn = st.button("ğŸš€ Start Download (å¼€å§‹ä¸‹è½½ - è‡ªåŠ¨ç¿»é¡µ)", type="primary")

if start_btn:
    with st.status("Downloading... (ä¸‹è½½ä¸­)", expanded=True) as status:
        if not token: status.update(label="Auth Failed (è®¤è¯å¤±è´¥)", state="error"); st.stop()
        progress_bar = st.progress(0); log_box = st.expander("Process Log (è¿è¡Œæ—¥å¿—)", expanded=True)
        total_ops = len(final_hs) * len(final_dirs); current_op = 0; stats = {"saved": 0}
        
        for hs in final_hs:
            for d in final_dirs:
                current_op += 1; progress_bar.progress(int(current_op/total_ops*100))
                
                # [NEW] ä½¿ç”¨ç”¨æˆ·å®šä¹‰çš„èµ·å§‹é¡µ
                page = start_page_val
                if page > 1:
                    log_box.info(f"â­ï¸ Resuming {hs} ({d}) from Page {page}...")
                
                has_more_data = True
                total_saved_for_this_hs = 0
                
                while has_more_data:
                    # è°ƒç”¨ utils, ä¼ å…¥ keyword
                    res = utils.fetch_tendata_api(hs, dl_date_range[0], dl_date_range[1], token, d, dl_origins, dl_dests, just_checking=False, page_no=page, keyword=api_keyword_str)
                    if res and str(res.get('code')) == '200':
                        saved_count, api_count = utils.save_to_supabase(res) # è°ƒç”¨ utils
                        total_saved_for_this_hs += saved_count
                        stats['saved'] += saved_count
                        log_box.write(f"ğŸ”„ HS {hs} ({d}) - P{page}: Fetched {api_count} records")
                        if api_count < 50: has_more_data = False
                        else: page += 1; time.sleep(0.3)
                    else:
                        # è®°å½•å…·ä½“çš„ä¸‹è½½é”™è¯¯
                        err_msg = res.get('msg', 'Unknown') if res else 'No Resp'
                        log_box.error(f"HS {hs}: Error - {err_msg}"); has_more_data = False
                
                if total_saved_for_this_hs > 0: log_box.success(f"âœ… HS {hs} ({d}) Done: Saved {total_saved_for_this_hs}")
                else: log_box.warning(f"HS {hs} ({d}): No Data")
        
        status.update(label="All Done (å…¨éƒ¨å®Œæˆ)", state="complete")
        st.success(f"ğŸ‰ Total Saved (ç´¯è®¡å…¥åº“): {stats['saved']} records")